"""
Slovensko - ORSR Provider (Live Scraping)
Hybridn√Ω model: Cache ‚Üí DB ‚Üí Live Scraping
"""

import re
from datetime import datetime, timedelta
from typing import Dict, Optional

import requests
from bs4 import BeautifulSoup

from services.cache import get, get_cache_key
from services.cache import set as cache_set
from services.database import CompanyCache, get_db_session
from services.proxy_rotation import get_proxy, mark_proxy_success, mark_proxy_failed


class OrsrProvider:
    """
    Provider pre z√≠skavanie d√°t z ORSR.sk cez live scraping.
    Pou≈æ√≠va hybridn√Ω model: Cache ‚Üí DB ‚Üí Live Scraping
    """

    CACHE_TTL = timedelta(hours=12)  # Cache na 12 hod√≠n
    DB_REFRESH_DAYS = 7  # Auto-refresh po 7 d≈àoch

    def __init__(self):
        self.session = requests.Session()
        # SSL overovanie pre ORSR u≈æ funguje korektne
        self.session.verify = True

    def lookup_by_ico(self, ico: str, force_refresh: bool = False) -> Optional[Dict]:
        """
        Vyhƒæad√° firmu podƒæa IƒåO s hybridn√Ωm modelom.

        Vrstvy:
        1. Cache (Redis/File) - najr√Ωchlej≈°ie
        2. DB - ak cache expirovala
        3. Live Scraping - ak DB je star√° alebo neexistuje

        Args:
            ico: 8-miestne slovensk√© IƒåO
            force_refresh: Vyn√∫ti≈• nov√Ω scraping

        Returns:
            Dict s d√°tami firmy alebo None
        """
        # 1. Cache vrstva (najr√Ωchlej≈°ia)
        if not force_refresh:
            cache_key = get_cache_key(f"orsr_sk_{ico}")
            cached_data = get(cache_key)
            if cached_data:
                print(f"‚úÖ Cache hit pre IƒåO {ico}")
                return cached_data

        # 2. DB vrstva
        # 2. DB vrstva
        try:
            with get_db_session() as db:
                if db:
                    company = (
                        db.query(CompanyCache)
                        .filter(
                            CompanyCache.identifier == ico, CompanyCache.country == "SK"
                        )
                        .first()
                    )

                    if company:
                        # Kontrola, ƒçi je DB z√°znam aktu√°lny
                        days_old = (datetime.utcnow() - company.last_synced_at).days

                        if days_old < self.DB_REFRESH_DAYS and not force_refresh:
                            print(f"‚úÖ DB hit pre IƒåO {ico} (star√© {days_old} dn√≠)")
                            data = (
                                company.company_data or company.data
                            )  # Fallback na legacy field
                            # Ulo≈æi≈• do cache
                            cache_set(cache_key, data, ttl=self.CACHE_TTL)
                            return data
                        else:
                            print(
                                f"‚ö†Ô∏è DB z√°znam star√Ω ({days_old} dn√≠), sp√∫≈°≈•am live scraping..."
                            )
        except Exception as e:
            print(f"‚ö†Ô∏è Chyba pri ƒç√≠tan√≠ z DB (ORSR): {e}")

        # 3. Live Scraping (najpomal≈°ie, ale najaktu√°lnej≈°ie)
        print(f"üîÑ Live scraping pre IƒåO {ico}...")
        live_data = self._scrape_orsr(ico)

        if live_data:
            # Ulo≈æi≈• do cache
            try:
                cache_set(cache_key, live_data, ttl=self.CACHE_TTL)
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to cache data: {e}")

            # Ulo≈æi≈• do DB
            try:
                with get_db_session() as db:
                    if db:
                        company = (
                            db.query(CompanyCache)
                            .filter(
                                CompanyCache.identifier == ico, CompanyCache.country == "SK"
                            )
                            .first()
                        )

                        if company:
                            # Aktualizova≈• existuj√∫ci z√°znam
                            company.company_data = live_data
                            company.data = live_data  # Legacy field
                            company.company_name = live_data.get("name")
                            company.risk_score = live_data.get("risk_score")
                            company.last_synced_at = datetime.utcnow()
                            company.updated_at = datetime.utcnow()
                        else:
                            # Vytvori≈• nov√Ω z√°znam
                            company = CompanyCache(
                                identifier=ico,
                                country="SK",
                                company_data=live_data,
                                data=live_data,  # Legacy field
                                company_name=live_data.get("name"),
                                risk_score=live_data.get("risk_score"),
                                last_synced_at=datetime.utcnow(),
                            )
                            db.add(company)

                        db.commit()
                        print(f"‚úÖ D√°ta ulo≈æen√© do DB pre IƒåO {ico}")
            except Exception as db_err:
                print(f"‚ö†Ô∏è Nepodarilo sa ulo≈æi≈• d√°ta do DB: {db_err}")

            return live_data

        return None

    def _scrape_orsr(self, ico: str) -> Optional[Dict]:
        """
        Vykon√° live scraping z ORSR.sk.

        Args:
            ico: 8-miestne slovensk√© IƒåO

        Returns:
            Dict s normalizovan√Ωmi d√°tami alebo None
        """
        try:
            # 1. Vyhƒæad√°vanie podƒæa IƒåO - Pou≈æi≈• spr√°vny endpoint hladaj_ico.asp
            search_url = f"https://www.orsr.sk/hladaj_ico.asp?ICO={ico}&SID=0"

            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            }

            proxy = get_proxy()
            if proxy:
                self.session.proxies = proxy
                print(f"üåê Pou≈æ√≠va sa proxy: {proxy.get('http') or proxy.get('https')}")

            try:
                response = self.session.get(search_url, headers=headers, timeout=30)
                if proxy: mark_proxy_success(proxy)
            except Exception as e:
                if proxy: mark_proxy_failed(proxy)
                print(f"‚ö†Ô∏è ORSR search error: {e}")
                return None

            response.encoding = 'windows-1250'
            
            if response.status_code != 200:
                print(f"‚ùå ORSR search failed: {response.status_code}")
                return None

            soup = BeautifulSoup(response.text, "html.parser")

            # 2. N√°js≈• link na detail v√Ωpisu
            # Hƒæad√°me link s title="Aktu√°lny v√Ωpis" aby sme ignorovali jazykov√© prep√≠naƒçe
            detail_link = soup.find("a", attrs={"title": "Aktu√°lny v√Ωpis"})
            
            if not detail_link:
                # Fallback: hƒæada≈• link v tabuƒæke v√Ωsledkov (ak title ch√Ωba)
                # Ignorujeme lan=en
                links = soup.find_all("a", href=lambda x: x and "vypis.asp?ID=" in x)
                for link in links:
                    href = link.get("href", "")
                    if "lan=en" not in href and "Aktu√°lny" in link.get_text():
                        detail_link = link
                        break

            if not detail_link:
                print(f"‚ö†Ô∏è IƒåO {ico} sa nena≈°lo v ORSR (link nen√°jden√Ω)")
                return None

            href = detail_link["href"]
            # Construct full URL - href m√¥≈æe by≈• relat√≠vny (och, orsr m√° niekedy 'vypis.asp' bez /)
            # ORSR links are usually "vypis.asp?ID=..."
            if href.startswith("/"):
                detail_url = f"https://www.orsr.sk{href}"
            else:
                detail_url = f"https://www.orsr.sk/{href}"
            
            # Remove encoding params if any (keep pure link)
            if "&amp;" in detail_url:
                detail_url = detail_url.replace("&amp;", "&")

            # 3. Stiahnu≈• detail v√Ωpisu
            detail_response = self.session.get(detail_url, headers=headers, timeout=30)
            detail_response.encoding = 'windows-1250'
            
            # DEBUG: Save HTML to file
            try:
                with open("debug_orsr_output.html", "w", encoding="utf-8") as f:
                    f.write(detail_response.text)
                print("DEBUG: Saved HTML to debug_orsr_output.html")
            except Exception as e:
                print(f"DEBUG: Failed to save HTML: {e}")

            print(f"DEBUG: Downloaded detail_url {detail_url}, Status: {detail_response.status_code}, Length: {len(detail_response.text)}")
            if detail_response.status_code != 200:
                print(f"‚ùå ORSR detail failed: {detail_response.status_code}")
                return None

            detail_soup = BeautifulSoup(detail_response.text, "html.parser")

            # 4. Parsova≈• HTML a extrahova≈• d√°ta
            data = self._parse_orsr_html(detail_soup, ico)
            
            if not data.get("name"):
                print(f"‚ùå Parsovanie zlyhalo - meno nen√°jden√© pre IƒåO {ico}")
                # Analyze why name wasn't found - dumping structure
                print("HTML Structure check:")
                print(f"HTML PREVIEW: {detail_soup.prettify()[:2000]}")
                tds = detail_soup.find_all("td")
                matching_tds = [td for td in tds if "Obchodn√© meno" in td.get_text()]
                print(f"Found {len(matching_tds)} tds with 'Obchodn√© meno'")
                for td in matching_tds:
                    print(f"TD Content: '{td.get_text(strip=True)}'")
                    sibling = td.find_next_sibling("td")
                    if sibling:
                        print(f"Sibling Content: '{sibling.get_text(strip=True)}'")
                    else:
                        print("No sibling found")
            else:
                print(f"‚úÖ Scraping √∫spe≈°n√Ω: {data.get('name')}")

            return data if data.get("name") else None

        except Exception as e:
            print(f"‚ùå Chyba pri scraping ORSR: {e}")
            return None

    def _parse_orsr_html(self, soup: BeautifulSoup, ico: str) -> Dict:
        """
        Parsuje HTML z ORSR v√Ωpisu a extrahuje d√°ta.
        """
        data = {
            "ico": ico,
            "country": "SK",
            "name": None,
            "legal_form": None,
            "address": None,
            "postal_code": None,
            "city": None,
            "region": None,
            "district": None,
            "executives": [],
            "shareholders": [],
            "founded": None,
            "status": "Akt√≠vna",
            "dic": None,
            "ic_dph": None,
        }

        # Helper to clean text
        def clean_text(text):
            if not text: return None
            # Remove (od: ...) and whitespace
            text = re.sub(r"\s*\(od:.*?\)", "", text)
            return text.strip()

        # Helper to find value next to label
        def get_value(label_pattern):
            # Find all TDs
            tds = soup.find_all("td")
            for td in tds:
                if td.get_text() and label_pattern in td.get_text():
                    sibling = td.find_next_sibling("td")
                    if sibling:
                        return clean_text(sibling.get_text(separator=" ", strip=True))
            return None

        # 1. N√°zov firmy
        data["name"] = get_value("Obchodn√© meno:")
        
        # 2. Pr√°vna forma
        data["legal_form"] = get_value("Pr√°vna forma:")
        
        # 3. S√≠dlo (Adresa)
        raw_address = get_value("S√≠dlo:")
        if raw_address:
            data["address"] = raw_address
            # Parse postal code and city
            postal_match = re.search(r"\b\d{3}\s?\d{2}\b", raw_address)
            if postal_match:
                data["postal_code"] = postal_match.group().replace(" ", "")
                # City is usually after postal code or at the end
                parts = raw_address.split(postal_match.group())
                if len(parts) > 1:
                    data["city"] = parts[1].strip().strip(",").strip()
                elif "," in raw_address:
                     data["city"] = raw_address.split(",")[-1].strip()

        # 4. ≈†tatut√°rny org√°n (Konatelia)
        stat_tds = [td for td in soup.find_all("td") if "≈†tatut√°rny org√°n:" in td.get_text()]
        if stat_tds:
            stat_td = stat_tds[0]
            names_td = stat_td.find_next_sibling("td")
            if names_td:
                content = names_td.get_text(separator="\n", strip=True)
                lines = content.split("\n")
                for line in lines:
                    line = clean_text(line)
                    if not line: continue
                    if "Vznik funkcie:" in line or "Sp√¥sob konania:" in line or "Typ:" in line: continue
                    if re.search(r"\d", line) and not re.search(r"Ing\.|Mgr\.|JUDr\.", line): continue # Address heuristic
                    if len(line) > 3:
                        data["executives"].append(line)

        # 5. Spoloƒçn√≠ci
        spol_tds = [td for td in soup.find_all("td") if "Spoloƒçn√≠ci:" in td.get_text()]
        if spol_tds:
            spol_td = spol_tds[0]
            names_td = spol_td.find_next_sibling("td")
            if names_td:
                content = names_td.get_text(separator="\n", strip=True)
                lines = content.split("\n")
                for line in lines:
                    line = clean_text(line)
                    if not line: continue
                    if "Vklad:" in line or "Splaten√©:" in line or "Osoba je" in line: continue
                    if re.search(r"\d", line) and not re.search(r"Ing\.|Mgr\.|JUDr\.", line): continue
                    if len(line) > 3:
                        data["shareholders"].append(line)

        # 6. De≈à z√°pisu
        data["founded"] = get_value("De≈à z√°pisu:")

        # Status check
        if "likvid√°cia" in str(soup).lower() or "konkurz" in str(soup).lower():
            data["status"] = "Likvid√°cia/Konkurz"
        
        # Deduplicate names
        data["executives"] = list(set(data["executives"]))
        data["shareholders"] = list(set(data["shareholders"]))


        
        # Obohatenie o geolok√°ciu (Kraj, Okres z PSƒå)
        if data.get("postal_code"):
            try:
                from services.sk_region_resolver import enrich_address_with_region
                region_data = enrich_address_with_region(data.get("address", ""), data["postal_code"])
                data["region"] = region_data.get("region")
                data["district"] = region_data.get("district")
            except: pass

        # Obohatenie o DIƒå/Iƒå DPH - Temporarily disabled for debugging speed
        # if not data.get("dic") and not data.get("ic_dph"):
             # print(f"üîç Hƒæad√°m DIƒå/Iƒå DPH pre IƒåO {ico}...")
             # ... (zrsr logic disabled)

        # Obohatenie o finanƒçn√© ukazovatele z RUZ - Temporarily disabled for debugging speed
        # try:
             # ... (ruz logic disabled)
        # except Exception as e:
             # print(f"‚ö†Ô∏è RUZ obohatenie zlyhalo: {e}")

        return data


# Singleton instance
_orsr_provider = None


def get_orsr_provider() -> OrsrProvider:
    """Vr√°ti singleton in≈°tanciu OrsrProvider."""
    global _orsr_provider
    if _orsr_provider is None:
        _orsr_provider = OrsrProvider()
    return _orsr_provider
